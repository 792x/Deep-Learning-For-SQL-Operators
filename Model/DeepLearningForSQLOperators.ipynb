{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# azureml-core of version 1.0.72 or higher is required\n",
        "from azureml.core import Workspace, Dataset, Experiment\n",
        "\n",
        "subscription_id = ''\n",
        "resource_group = ''\n",
        "workspace_name = ''\n",
        "\n",
        "workspace = Workspace(subscription_id, resource_group, workspace_name)\n",
        "\n",
        "experiment_name = ''\n",
        "experiment = Experiment(workspace, name=experiment_name)\n",
        "\n",
        "# dataset = Dataset.get_by_name(workspace, name='PhoneLabs')\n",
        "# dataset_mounted = dataset.as_mount()\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1625155415290
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "import csv\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import time\n",
        "import math\n",
        "import bottleneck as bn\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 4,
          "status": "ok",
          "timestamp": 1625141138680,
          "user": {
            "displayName": "Casper S",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgBWNg-LIJSRsGFg-KH-8zFdztG9qjoiJIbOsd4kA=s64",
            "userId": "09157385483834993232"
          },
          "user_tz": -120
        },
        "gather": {
          "logged": 1625155416262
        },
        "id": "_MY7N12ZYAyk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 461,
          "status": "ok",
          "timestamp": 1625141139715,
          "user": {
            "displayName": "Casper S",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgBWNg-LIJSRsGFg-KH-8zFdztG9qjoiJIbOsd4kA=s64",
            "userId": "09157385483834993232"
          },
          "user_tz": -120
        },
        "gather": {
          "logged": 1625155416335
        },
        "id": "VJa1f_wkYFfq",
        "outputId": "f97afd85-751d-4374-8f15-1eeea1751e37"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cd '/content/drive/My Drive/Colab Notebooks/Query Operators'"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 10,
          "status": "ok",
          "timestamp": 1625141139716,
          "user": {
            "displayName": "Casper S",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgBWNg-LIJSRsGFg-KH-8zFdztG9qjoiJIbOsd4kA=s64",
            "userId": "09157385483834993232"
          },
          "user_tz": -120
        },
        "gather": {
          "logged": 1625155416463
        },
        "id": "tAtwgwYxph7V",
        "outputId": "1402d476-8564-4151-c8a4-868932b7e346"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_iter = 5\n",
        "hidden_size = 32\n",
        "num_layers = 1\n",
        "\n",
        "use_dependency_graph = True\n",
        "use_gamma = True\n",
        "use_dependency_graph_inside_model = False\n",
        "\n",
        "# only one can be set 1\n",
        "use_embedding = 1\n",
        "use_linear_reduction = 0\n",
        "###\n",
        "atten_decoder = 1\n",
        "use_dropout = 0\n",
        "use_average_embedding = 1\n",
        "\n",
        "weight = 10\n",
        "labmda = 0\n",
        "topk_labels = 3\n",
        "\n",
        "# It should be the same as the reductioned input in decoder's cat function\n",
        "\n",
        "teacher_forcing_ratio = 0\n",
        "MAX_LENGTH = 1000\n",
        "learning_rate = 0.001\n",
        "optimizer_option = 2\n",
        "print_val = 100\n",
        "use_cuda = torch.cuda.is_available()\n",
        "use_cuda = True\n",
        "\n",
        "print(torch.cuda.get_device_name(0))\n",
        "print(torch.cuda.device_count())\n",
        "print(f\"Using cuda: {use_cuda}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 7,
          "status": "ok",
          "timestamp": 1625141139716,
          "user": {
            "displayName": "Casper S",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgBWNg-LIJSRsGFg-KH-8zFdztG9qjoiJIbOsd4kA=s64",
            "userId": "09157385483834993232"
          },
          "user_tz": -120
        },
        "gather": {
          "logged": 1625155416594
        },
        "id": "UEu2kyOEYHpK",
        "outputId": "dee75205-451f-4a80-a988-0f4bb98a64f4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
        "\n",
        "def top_n_indexes(arr, n):\n",
        "    idx = bn.argpartition(arr, arr.size - n, axis=None)[-n:]\n",
        "    width = arr.shape[1]\n",
        "    return [divmod(i, width) for i in idx]\n",
        "\n",
        "\n",
        "def get_precision_recall_Fscore(groundtruth, pred):\n",
        "    a = groundtruth\n",
        "    b = pred\n",
        "    correct = 0\n",
        "    truth = 0\n",
        "    positive = 0\n",
        "\n",
        "    for idx in range(len(a)):\n",
        "        if a[idx] == 1:\n",
        "            truth += 1\n",
        "            if b[idx] == 1:\n",
        "                correct += 1\n",
        "        if b[idx] == 1:\n",
        "            positive += 1\n",
        "\n",
        "    flag = 0\n",
        "    if 0 == positive:\n",
        "        precision = 0\n",
        "        flag = 1\n",
        "        # print('postivie is 0')\n",
        "    else:\n",
        "        precision = correct / positive\n",
        "    if 0 == truth:\n",
        "        recall = 0\n",
        "        flag = 1\n",
        "        # print('recall is 0')\n",
        "    else:\n",
        "        recall = correct / truth\n",
        "\n",
        "    if flag == 0 and precision + recall > 0:\n",
        "        F = 2 * precision * recall / (precision + recall)\n",
        "    else:\n",
        "        F = 0\n",
        "    return precision, recall, F, correct\n",
        "\n",
        "\n",
        "def get_F_score(prediction, test_Y):\n",
        "    jaccard_similarity = []\n",
        "    prec = []\n",
        "    rec = []\n",
        "\n",
        "    count = 0\n",
        "    for idx in range(len(test_Y)):\n",
        "        pred = prediction[idx]\n",
        "        T = 0\n",
        "        P = 0\n",
        "        correct = 0\n",
        "        for id in range(len(pred)):\n",
        "            if test_Y[idx][id] == 1:\n",
        "                T = T + 1\n",
        "                if pred[id] == 1:\n",
        "                    correct = correct + 1\n",
        "            if pred[id] == 1:\n",
        "                P = P + 1\n",
        "\n",
        "        if P == 0 or T == 0:\n",
        "            continue\n",
        "        precision = correct / P\n",
        "        recall = correct / T\n",
        "        prec.append(precision)\n",
        "        rec.append(recall)\n",
        "        if correct == 0:\n",
        "            jaccard_similarity.append(0)\n",
        "        else:\n",
        "            jaccard_similarity.append(2 * precision * recall / (precision + recall))\n",
        "        count = count + 1\n",
        "\n",
        "    print(\n",
        "        'average precision: ' + str(np.mean(prec)))\n",
        "    print('average recall : ' + str(\n",
        "        np.mean(rec)))\n",
        "    print('average F score: ' + str(\n",
        "        np.mean(jaccard_similarity)))\n",
        "\n",
        "\n",
        "def get_DCG(groundtruth, pred_rank_list, k):\n",
        "    count = 0\n",
        "    dcg = 0\n",
        "    for pred in pred_rank_list:\n",
        "        if count >= k:\n",
        "            break\n",
        "        if groundtruth[pred] == 1:\n",
        "            dcg += (1) / math.log2(count + 1 + 1)\n",
        "        count += 1\n",
        "\n",
        "    return dcg\n",
        "\n",
        "\n",
        "def get_NDCG(groundtruth, pred_rank_list, k):\n",
        "    count = 0\n",
        "    dcg = 0\n",
        "    for pred in pred_rank_list:\n",
        "        if count >= k:\n",
        "            break\n",
        "        if groundtruth[pred] == 1:\n",
        "            dcg += (1) / math.log2(count + 1 + 1)\n",
        "        count += 1\n",
        "    idcg = 0\n",
        "    num_real_item = np.sum(groundtruth)\n",
        "    num_item = int(min(num_real_item, k))\n",
        "    for i in range(num_item):\n",
        "        idcg += (1) / math.log2(i + 1 + 1)\n",
        "    ndcg = dcg / idcg\n",
        "    return ndcg\n",
        "\n",
        "\n",
        "def get_HT(groundtruth, pred_rank_list, k):\n",
        "    count = 0\n",
        "    for pred in pred_rank_list:\n",
        "        if count >= k:\n",
        "            break\n",
        "        if groundtruth[pred] == 1:\n",
        "            return 1\n",
        "        count += 1\n",
        "\n",
        "    return 0\n",
        "def minMax(x):\n",
        "    return pd.Series(index=['min','max'],data=[x.min(),x.max()])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 5,
          "status": "ok",
          "timestamp": 1625141139717,
          "user": {
            "displayName": "Casper S",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgBWNg-LIJSRsGFg-KH-8zFdztG9qjoiJIbOsd4kA=s64",
            "userId": "09157385483834993232"
          },
          "user_tz": -120
        },
        "gather": {
          "logged": 1625155416671
        },
        "id": "3Xx1wKR_Y84y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.reduction = nn.Linear(input_size, hidden_size)\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.time_embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.time_weight = nn.Linear(input_size, input_size)\n",
        "        if use_embedding or use_linear_reduction:\n",
        "            self.gru = nn.GRU(hidden_size, hidden_size, num_layers)\n",
        "        else:\n",
        "            self.gru = nn.GRU(input_size, hidden_size, num_layers)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        if use_embedding:\n",
        "            input_list = Variable(torch.LongTensor(input).view(-1, 1))\n",
        "            if use_cuda:\n",
        "                input_list = input_list.cuda()\n",
        "            average_embedding = Variable(torch.zeros(hidden_size)).view(1, 1, -1)\n",
        "            # sum_embedding = Variable(torch.zeros(hidden_size)).view(1,1,-1)\n",
        "            vectorized_input = Variable(torch.zeros(self.input_size)).view(-1)\n",
        "            if use_cuda:\n",
        "                average_embedding = average_embedding.cuda()\n",
        "                # sum_embedding = sum_embedding.cuda()\n",
        "                vectorized_input = vectorized_input.cuda()\n",
        "\n",
        "            for ele in input_list:\n",
        "                embedded = self.embedding(ele).view(1, 1, -1)\n",
        "                tmp = average_embedding.clone()\n",
        "                average_embedding = tmp + embedded\n",
        "                # embedded = self.time_embedding(ele).view(1, 1, -1)\n",
        "                # tmp = sum_embedding.clone()\n",
        "                # sum_embedding = tmp + embedded\n",
        "                vectorized_input[ele] = 1\n",
        "\n",
        "            # normalize_length = Variable(torch.LongTensor(len(idx_list)))\n",
        "            # if use_cuda:\n",
        "            #     normalize_length = normalize_length.cuda()\n",
        "            if use_average_embedding:\n",
        "                tmp = [1] * hidden_size\n",
        "                length = Variable(torch.FloatTensor(tmp))\n",
        "                if use_cuda:\n",
        "                    length = length.cuda()\n",
        "                # for idx in range(hidden_size):\n",
        "                real_ave = average_embedding.view(-1) / length\n",
        "                average_embedding = real_ave.view(1, 1, -1)\n",
        "\n",
        "            embedding = average_embedding\n",
        "        else:\n",
        "            tensorized_input = torch.from_numpy(input).clone().type(torch.FloatTensor)\n",
        "            inputs = Variable(torch.unsqueeze(tensorized_input, 0).view(1, -1))\n",
        "            if use_cuda:\n",
        "                inputs = inputs.cuda()\n",
        "            if use_linear_reduction == 1:\n",
        "                reduced_input = self.reduction(inputs)\n",
        "            else:\n",
        "                reduced_input = inputs\n",
        "\n",
        "            embedding = torch.unsqueeze(reduced_input, 0)\n",
        "\n",
        "        output, hidden = self.gru(embedding, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        result = Variable(torch.zeros(num_layers, 1, self.hidden_size))\n",
        "        if use_cuda:\n",
        "            return result.cuda()\n",
        "        else:\n",
        "            return result\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 5,
          "status": "ok",
          "timestamp": 1625141139717,
          "user": {
            "displayName": "Casper S",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgBWNg-LIJSRsGFg-KH-8zFdztG9qjoiJIbOsd4kA=s64",
            "userId": "09157385483834993232"
          },
          "user_tz": -120
        },
        "gather": {
          "logged": 1625155416895
        },
        "id": "icsVwvODYL4q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, num_layers, dropout_p=0.2, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        if use_embedding or use_linear_reduction:\n",
        "            self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "            self.attn1 = nn.Linear(self.hidden_size + output_size, self.hidden_size)\n",
        "        else:\n",
        "            self.attn = nn.Linear(self.hidden_size + self.output_size, self.output_size)\n",
        "\n",
        "        if use_embedding or use_linear_reduction:\n",
        "            self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "            self.attn_combine3 = nn.Linear(self.hidden_size * 2 + output_size, self.hidden_size)\n",
        "        else:\n",
        "            self.attn_combine = nn.Linear(self.hidden_size + self.output_size, self.hidden_size)\n",
        "        self.attn_combine5 = nn.Linear(self.output_size, self.output_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.reduction = nn.Linear(self.output_size, self.hidden_size)\n",
        "        if use_embedding or use_linear_reduction:\n",
        "            self.gru = nn.GRU(hidden_size, hidden_size, num_layers)\n",
        "        else:\n",
        "            self.gru = nn.GRU(hidden_size, hidden_size, num_layers)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs, history_record, last_hidden):\n",
        "        if use_embedding:\n",
        "            input_list = Variable(torch.LongTensor(input).view(-1, 1))\n",
        "            if use_cuda:\n",
        "                input_list = input_list.cuda()\n",
        "            average_embedding = Variable(torch.zeros(hidden_size)).view(1, 1, -1)\n",
        "            if use_cuda:\n",
        "                average_embedding = average_embedding.cuda()\n",
        "\n",
        "            for ele in input_list:\n",
        "                embedded = self.embedding(ele).view(1, 1, -1)\n",
        "                tmp = average_embedding.clone()\n",
        "                average_embedding = tmp + embedded\n",
        "\n",
        "            if use_average_embedding:\n",
        "                tmp = [1] * hidden_size\n",
        "                length = Variable(torch.FloatTensor(tmp))\n",
        "                if use_cuda:\n",
        "                    length = length.cuda()\n",
        "                # for idx in range(hidden_size):\n",
        "                real_ave = average_embedding.view(-1) / length\n",
        "                average_embedding = real_ave.view(1, 1, -1)\n",
        "\n",
        "            embedding = average_embedding\n",
        "        else:\n",
        "            tensorized_input = torch.from_numpy(input).clone().type(torch.FloatTensor)\n",
        "            inputs = Variable(torch.unsqueeze(tensorized_input, 0).view(1, -1))\n",
        "            if use_cuda:\n",
        "                inputs = inputs.cuda()\n",
        "            if use_linear_reduction == 1:\n",
        "                reduced_input = self.reduction(inputs)\n",
        "            else:\n",
        "                reduced_input = inputs\n",
        "\n",
        "            embedding = torch.unsqueeze(reduced_input, 0)\n",
        "\n",
        "        if use_dropout:\n",
        "            droped_ave_embedded = self.dropout(embedding)\n",
        "        else:\n",
        "            droped_ave_embedded = embedding\n",
        "\n",
        "        history_context = Variable(torch.FloatTensor(history_record).view(1, -1))\n",
        "        if use_cuda:\n",
        "            history_context = history_context.cuda()\n",
        "\n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((droped_ave_embedded[0], hidden[0]), 1)), dim=1)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        element_attn_weights = F.softmax(\n",
        "            self.attn1(torch.cat((history_context, hidden[0]), 1)), dim=1)\n",
        "\n",
        "        # attn_applied = torch.bmm(element_attn_weights.unsqueeze(0),encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        # attn_embedd = element_attn_weights * droped_ave_embedded[0]\n",
        "\n",
        "        output = torch.cat((droped_ave_embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "        # output = torch.cat((droped_ave_embedded[0], attn_applied[0], time_coef.unsqueeze(0)), 1)\n",
        "        # output = self.attn_combine3(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "\n",
        "        linear_output = self.out(output[0])\n",
        "        # output_user_item = F.softmax(linear_output)\n",
        "\n",
        "        value = torch.sigmoid(self.attn_combine5(history_context).unsqueeze(0))\n",
        "\n",
        "        one_vec = Variable(torch.ones(self.output_size).view(1, -1))\n",
        "        if use_cuda:\n",
        "            one_vec = one_vec.cuda()\n",
        "\n",
        "        # ones_set = torch.index_select(value[0,0], 1, ones_idx_set[:, 1])\n",
        "        res = history_context.clone()\n",
        "        res[history_context != 0] = 1\n",
        "        \n",
        "        if use_gamma:\n",
        "            output = F.softmax(linear_output * (one_vec - res * value[0]) + history_context * value[0], dim=1)\n",
        "        else:\n",
        "            output = F.softmax(linear_output, dim=1)\n",
        "\n",
        "        return output.view(1, -1), hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        result = Variable(torch.zeros(num_layers, 1, self.hidden_size))\n",
        "        if use_cuda:\n",
        "            return result.cuda()\n",
        "        else:\n",
        "            return result"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 429,
          "status": "ok",
          "timestamp": 1625141140142,
          "user": {
            "displayName": "Casper S",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgBWNg-LIJSRsGFg-KH-8zFdztG9qjoiJIbOsd4kA=s64",
            "userId": "09157385483834993232"
          },
          "user_tz": -120
        },
        "gather": {
          "logged": 1625155417070
        },
        "id": "kadWJ7qPYNVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class custom_MultiLabelLoss_torch(nn.modules.loss._Loss):\n",
        "    def __init__(self):\n",
        "        super(custom_MultiLabelLoss_torch, self).__init__()\n",
        "\n",
        "    def forward(self, pred, target, weights):\n",
        "        mseloss = torch.sum(weights * torch.pow((pred - target), 2))\n",
        "        pred = pred.data\n",
        "        target = target.data\n",
        "        \n",
        "        ones_idx_set = (target == 1).nonzero()\n",
        "        zeros_idx_set = (target == 0).nonzero()\n",
        "        \n",
        "        ones_set = torch.index_select(pred, 1, ones_idx_set[:, 1])\n",
        "        zeros_set = torch.index_select(pred, 1, zeros_idx_set[:, 1])\n",
        "        \n",
        "        repeat_ones = ones_set.repeat(1, zeros_set.shape[1])\n",
        "        repeat_zeros_set = torch.transpose(zeros_set.repeat(ones_set.shape[1], 1), 0, 1).clone()\n",
        "        repeat_zeros = repeat_zeros_set.reshape(1, -1)\n",
        "        difference_val = -(repeat_ones - repeat_zeros)\n",
        "        exp_val = torch.exp(difference_val)\n",
        "        exp_loss = torch.sum(exp_val)\n",
        "        normalized_loss = exp_loss / (zeros_set.shape[1] * ones_set.shape[1])\n",
        "        set_loss = Variable(torch.FloatTensor([labmda * normalized_loss]), requires_grad=True)\n",
        "        if use_cuda:\n",
        "            set_loss = set_loss.cuda()\n",
        "        loss = mseloss + set_loss\n",
        "        # loss = mseloss\n",
        "        return loss"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 5,
          "status": "ok",
          "timestamp": 1625141140143,
          "user": {
            "displayName": "Casper S",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgBWNg-LIJSRsGFg-KH-8zFdztG9qjoiJIbOsd4kA=s64",
            "userId": "09157385483834993232"
          },
          "user_tz": -120
        },
        "gather": {
          "logged": 1625155417277
        },
        "id": "TwOHVcDAYX7R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(input_variable, target_variable, encoder, decoder, codes_inverse_freq, encoder_optimizer, decoder_optimizer,\n",
        "          criterion, output_size, next_k_step, update_params, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = len(input_variable)\n",
        "    target_length = len(target_variable)\n",
        "\n",
        "    encoder_outputs = Variable(torch.zeros(max_length, encoder.hidden_size))\n",
        "    if use_cuda:\n",
        "        encoder_outputs = encoder_outputs.cuda()\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    history_record = np.zeros(output_size)\n",
        "    for ei in range(input_length - 1):\n",
        "        if ei == 0:\n",
        "            continue\n",
        "        for ele in input_variable[ei]:\n",
        "            history_record[ele] += 1 / (input_length - 2)\n",
        "\n",
        "    for ei in range(input_length - 1):\n",
        "        if ei == 0:\n",
        "            continue\n",
        "        encoder_output, encoder_hidden = encoder(input_variable[ei], encoder_hidden)\n",
        "        encoder_outputs[ei - 1] = encoder_output[0][0]\n",
        "\n",
        "    last_input = input_variable[input_length - 2]\n",
        "    decoder_hidden = encoder_hidden\n",
        "    last_hidden = encoder_hidden\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    topk = 1\n",
        "\n",
        "    decoder_input = last_input\n",
        "    for di in range(next_k_step):\n",
        "\n",
        "        if atten_decoder:\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs, history_record, last_hidden)\n",
        "        else:\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden)\n",
        "        topv, topi = decoder_output.data.topk(topk)\n",
        "        ni = topi[0][0]\n",
        "\n",
        "        # activation_bound\n",
        "        # topk_labels\n",
        "        # target_neg = zero2neg(target_variable[di])\n",
        "\n",
        "        vectorized_target = np.zeros(output_size)\n",
        "        for idx in target_variable[di + 1]:\n",
        "            vectorized_target[idx] = 1\n",
        "\n",
        "        target = Variable(torch.FloatTensor(vectorized_target).view(1, -1))\n",
        "        if use_cuda:\n",
        "            target = target.cuda()\n",
        "        weights = Variable(torch.FloatTensor(codes_inverse_freq).view(1, -1))\n",
        "        if use_cuda:\n",
        "            weights = weights.cuda()\n",
        "        tt = criterion(decoder_output, target, weights)\n",
        "        # tt = criterion(decoder_output, target)\n",
        "        # tt = torch.sum(weights*torch.pow((decoder_output - target),2))\n",
        "        loss += tt\n",
        "        decoder_input = target_variable[di + 1]\n",
        "        # loss += multilable_loss(decoder_output, target)\n",
        "\n",
        "    # encoder_optimizer.zero_grad()\n",
        "    # decoder_optimizer.zero_grad()\n",
        "    if update_params:\n",
        "        loss.backward()\n",
        "\n",
        "        encoder_optimizer.step()\n",
        "        decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 4,
          "status": "ok",
          "timestamp": 1625141140143,
          "user": {
            "displayName": "Casper S",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgBWNg-LIJSRsGFg-KH-8zFdztG9qjoiJIbOsd4kA=s64",
            "userId": "09157385483834993232"
          },
          "user_tz": -120
        },
        "gather": {
          "logged": 1625155417446
        },
        "id": "LyTJ96N_Y3k_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_after_subplot(ax: plt.Axes, group: str, x_label: str):\n",
        "    if group == 'folds':\n",
        "        ax.set_xlabel(\"fold\")\n",
        "    if group == 'epochs':\n",
        "        ax.set_xlabel(\"epochs\")\n",
        "   \n",
        "\n",
        "def trainIters(X_train, y_train, X_val, y_val, output_size, encoder, decoder, model_id, weights,\n",
        "                       next_k_step, n_iters, num_set_elements, implied_elements, print_every=100):\n",
        "    \n",
        "    start = time.time()\n",
        "    full_training_losses = []\n",
        "    full_validation_losses = []\n",
        "\n",
        "    epoch_metrics = {}\n",
        "    fold_metrics = {}\n",
        "\n",
        "\n",
        "    if optimizer_option == 1:\n",
        "        encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "        decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    elif optimizer_option == 2:\n",
        "        # encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=learning_rate, betas=(0.9, 0.98), eps=1e-09, weight_decay=0)\n",
        "        # encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=learning_rate, betas=(0.88, 0.95), eps=1e-08, weight_decay=0)\n",
        "        encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=learning_rate, betas=(0.9, 0.98), eps=1e-11,\n",
        "                                             weight_decay=0)\n",
        "        decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=learning_rate, betas=(0.9, 0.98), eps=1e-11,\n",
        "                                             weight_decay=0)\n",
        "    elif optimizer_option == 3:\n",
        "        encoder_optimizer = torch.optim.RMSprop(encoder.parameters(), lr=learning_rate, alpha=0.99, eps=1e-08,\n",
        "                                                weight_decay=0, momentum=0, centered=False)\n",
        "        decoder_optimizer = torch.optim.RMSprop(decoder.parameters(), lr=learning_rate, alpha=0.99, eps=1e-08,\n",
        "                                                weight_decay=0, momentum=0, centered=False)\n",
        "    elif optimizer_option == 4:\n",
        "        encoder_optimizer = torch.optim.Adadelta(encoder.parameters(), lr=learning_rate, rho=0.9, eps=1e-06,\n",
        "                                                 weight_decay=0)\n",
        "        decoder_optimizer = torch.optim.Adadelta(decoder.parameters(), lr=learning_rate, rho=0.9, eps=1e-06,\n",
        "                                                 weight_decay=0)\n",
        "    \n",
        "    total_iter = 0\n",
        "\n",
        "    criterion = custom_MultiLabelLoss_torch()\n",
        "    # criterion = nn.MSELoss()\n",
        "    # criterion = nn.NLLLoss()\n",
        "    # criterion = nn.MultiLabelSoftMarginLoss(size_average=False)\n",
        "    # criterion = nn.BCELoss()\n",
        "\n",
        "    for j in range(n_iters):\n",
        "        logs = {}\n",
        "        training_loss_total = 0.0\n",
        "        validation_loss_total = 0.0\n",
        "        print(\"Epoch:\", j)\n",
        "\n",
        "        # Training\n",
        "        for iter in range(len(y_train)):\n",
        "            print(\"\\r\" +'Train iter ', iter+1, ' out of ', len(y_train), \"(\" + str(int((iter+1) / len(y_train) * 100)) + \"%)\" ,end=\"\")  if (iter+1) % print_every == 0 else 0\n",
        "            input_variable = X_train[iter] # past sequence\n",
        "            target_variable = y_train[iter] # future sequence\n",
        "\n",
        "            loss = train(input_variable, target_variable, encoder,\n",
        "                        decoder, weights, encoder_optimizer, decoder_optimizer, criterion, output_size,\n",
        "                        next_k_step, update_params=True)\n",
        "            training_loss_total += loss\n",
        "            full_training_losses.append(loss)\n",
        "\n",
        "            total_iter += 1\n",
        "        print(\"\\r\")\n",
        "\n",
        "        # Validation\n",
        "        with torch.no_grad():\n",
        "            recall, ndcg, hr, prec, f1 = evaluate(X_val, y_val, encoder, decoder, output_size, next_k_step, num_set_elements, implied_elements) \n",
        "            \n",
        "            for iter in range(len(y_val)):\n",
        "                print(\"\\r\" +'Validation iter ', iter+1, ' out of ', len(y_val),\"(\" + str(int((iter+1) / len(y_val) * 100)) + \"%)\" ,end=\"\") if (iter+1) % print_every == 0 else 0\n",
        "                input_variable = X_val[iter] # past sequence\n",
        "                target_variable = y_val[iter] # future sequence\n",
        "\n",
        "                loss = train(input_variable, target_variable, encoder,\n",
        "                            decoder, weights, encoder_optimizer, decoder_optimizer, criterion, output_size,\n",
        "                            next_k_step, update_params=False)\n",
        "                validation_loss_total += loss\n",
        "                full_validation_losses.append(loss)\n",
        "            print(\"\\r\")\n",
        "            print('Epoch avg val recall:' + str(recall))\n",
        "            print('Epoch avg val precision:' + str(prec))\n",
        "            print('Epoch avg val f1:' + str(f1))\n",
        "            print('Epoch avg val ndcg:' + str(ndcg))\n",
        "            print('Epoch avg val hr:' + str(hr))\n",
        "\n",
        "        # Metrics per epoch\n",
        "        training_loss_avg = training_loss_total / len(y_train)\n",
        "        validation_loss_avg = validation_loss_total / len(y_val)\n",
        "        print('%s (%d %d%%) train %.6f val %.6f' % (timeSince(start, total_iter / (n_iters * len(y_train))), total_iter, total_iter / (n_iters * len(y_train)) * 100,training_loss_avg, validation_loss_avg))\n",
        "        \n",
        "        # lc = pd.DataFrame({\"loss\": full_training_losses})\n",
        "        # lc.plot(lw=2, title=\"training loss\");\n",
        "        # plt.xlabel('iterations');\n",
        "\n",
        "        filepath = './models/encoder' + (model_id) + '_model_epoch' + str(int(j))\n",
        "        torch.save(encoder, filepath)\n",
        "        filepath = './models/decoder' + (model_id) + '_model_epoch' + str(int(j))\n",
        "        torch.save(decoder, filepath)\n",
        "        print('Finish epoch: ' + str(j))\n",
        "        print('Model is saved.')\n",
        "        sys.stdout.flush()\n",
        "    # showPlot(plot_losses)\n",
        "    # print('The loss: ' + str(print_loss_total))\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 4,
          "status": "ok",
          "timestamp": 1625141140143,
          "user": {
            "displayName": "Casper S",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgBWNg-LIJSRsGFg-KH-8zFdztG9qjoiJIbOsd4kA=s64",
            "userId": "09157385483834993232"
          },
          "user_tz": -120
        },
        "gather": {
          "logged": 1625155417704
        },
        "id": "pFN6BTSQY_Qd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_sim = []\n",
        "pair_cosine_sim = []\n",
        "\n",
        "\n",
        "def decoding_next_k_step(encoder, decoder, input_variable, target_variable, output_size, k, num_set_elements, implied_elements):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    input_length = len(input_variable)\n",
        "    encoder_outputs = Variable(torch.zeros(MAX_LENGTH, encoder.hidden_size))\n",
        "    if use_cuda:\n",
        "        encoder_outputs = encoder_outputs.cuda()\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    history_record = np.zeros(output_size)\n",
        "    count = 0\n",
        "    for ei in range(input_length - 1):\n",
        "        if ei == 0:\n",
        "            continue\n",
        "        for ele in input_variable[ei]:\n",
        "            history_record[ele] += 1\n",
        "        count += 1\n",
        "\n",
        "    history_record = history_record / count\n",
        "\n",
        "    for ei in range(input_length - 1):\n",
        "        if ei == 0:\n",
        "            continue\n",
        "        encoder_output, encoder_hidden = encoder(input_variable[ei], encoder_hidden)\n",
        "        encoder_outputs[ei - 1] = encoder_output[0][0]\n",
        "\n",
        "        for ii in range(k):\n",
        "            vectorized_target = np.zeros(output_size)\n",
        "            for idx in target_variable[ii + 1]:\n",
        "                vectorized_target[idx] = 1\n",
        "\n",
        "            vectorized_input = np.zeros(output_size)\n",
        "            for idx in input_variable[ei]:\n",
        "                vectorized_input[idx] = 1\n",
        "\n",
        "    decoder_input = input_variable[input_length - 2]\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "    last_hidden = decoder_hidden\n",
        "    # Without teacher forcing: use its own predictions as the next input\n",
        "    num_str = 0\n",
        "    topk = 400\n",
        "    decoded_vectors = []\n",
        "    prob_vectors = []\n",
        "    cout = 0\n",
        "    for di in range(k):\n",
        "        if atten_decoder:\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs, history_record, last_hidden)\n",
        "        else:\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden)\n",
        "        topv, topi = decoder_output.data.topk(topk)\n",
        "        ni = topi[0][0]\n",
        "\n",
        "        vectorized_target = np.zeros(output_size)\n",
        "        for idx in target_variable[di + 1]:\n",
        "            vectorized_target[idx] = 1\n",
        "\n",
        "        # target_topi = vectorized_target.argsort()[::-1][:topk]\n",
        "        # activation_bound\n",
        "\n",
        "        count = 0\n",
        "        start_idx = -1\n",
        "        end_idx = output_size\n",
        "        if num_set_elements > 0:\n",
        "            pick_num = num_set_elements\n",
        "        else:\n",
        "            pick_num = np.sum(vectorized_target)\n",
        "            # print(pick_num)\n",
        "\n",
        "        tmp = []\n",
        "        for ele in range(len(topi[0])):\n",
        "            if count >= pick_num:\n",
        "                break\n",
        "            tmp.append(topi[0][ele])\n",
        "            count += 1\n",
        "\n",
        "        res = []\n",
        "\n",
        "        for i, ele in enumerate(tmp):\n",
        "            res.append(ele)\n",
        "            if i < len(tmp) and use_dependency_graph:\n",
        "                implicit_elements = implied_elements[str(ele.item())]\n",
        "                for impl_ele in implicit_elements:\n",
        "                    t = torch.LongTensor([impl_ele]).squeeze()\n",
        "                    if use_cuda:\n",
        "                        t = t.cuda()\n",
        "                    res.append(t)\n",
        "\n",
        "        decoded_vectors.append(res)\n",
        "        \n",
        "        if use_dependency_graph_inside_model: \n",
        "            decoder_input = res\n",
        "        else:\n",
        "            decoder_input = tmp\n",
        "        \n",
        "        res2 = []\n",
        "        for i in range(topk):\n",
        "            res2.append(topi[0][i])\n",
        "        prob_vectors.append(res2)\n",
        "\n",
        "    return decoded_vectors, prob_vectors\n",
        "\n",
        "\n",
        "# def vectorized2set(vector, set):\n",
        "#     print('vector length', len(vector))\n",
        "#     i in range(len(vector)):\n",
        "#         if vector[i] == 1:\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 3,
          "status": "ok",
          "timestamp": 1625141140510,
          "user": {
            "displayName": "Casper S",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgBWNg-LIJSRsGFg-KH-8zFdztG9qjoiJIbOsd4kA=s64",
            "userId": "09157385483834993232"
          },
          "user_tz": -120
        },
        "gather": {
          "logged": 1625155417928
        },
        "id": "D8nG8mdIZDG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(X_test, y_test, encoder, decoder, output_size, next_k_step, num_set_elements, implied_elements, shouldPrint=False):\n",
        "    prec = []\n",
        "    rec = []\n",
        "    F = []\n",
        "    prec1 = []\n",
        "    rec1 = []\n",
        "    F1 = []\n",
        "    prec2 = []\n",
        "    rec2 = []\n",
        "    F2 = []\n",
        "    prec3 = []\n",
        "    rec3 = []\n",
        "    F3 = []\n",
        "\n",
        "    NDCG = []\n",
        "    n_hit = 0\n",
        "    count = 0\n",
        "    \n",
        "    inference_time = []\n",
        "\n",
        "    for iter in range(len(y_test)):\n",
        "        input_variable = X_test[iter]\n",
        "        target_variable = y_test[iter]\n",
        "\n",
        "        count += 1\n",
        "        start = time.process_time()\n",
        "        output_vectors, prob_vectors = decoding_next_k_step(encoder, decoder, input_variable, target_variable,\n",
        "                                                            output_size, next_k_step, num_set_elements, implied_elements)\n",
        "        elapsed = time.process_time() - start\n",
        "        inference_time.append(elapsed)\n",
        "        hit = 0\n",
        "        # for each set\n",
        "        for idx in range(len(output_vectors)):\n",
        "            # for idx in [2]:\n",
        "\n",
        "            vectorized_target = np.zeros(output_size)\n",
        "            for ii in target_variable[1 + idx]:\n",
        "                vectorized_target[ii] = 1\n",
        "\n",
        "\n",
        "            \n",
        "\n",
        "            vectorized_output = np.zeros(output_size)\n",
        "            prediction = [int(t.item()) for t in output_vectors[idx]]\n",
        "            for ii in output_vectors[idx]:\n",
        "                vectorized_output[ii] = 1\n",
        "\n",
        "            precision, recall, Fscore, correct = get_precision_recall_Fscore(vectorized_target, vectorized_output)\n",
        "            \n",
        "            prec.append(precision)\n",
        "            rec.append(recall)\n",
        "            F.append(Fscore)\n",
        "            if idx == 0:\n",
        "                prec1.append(precision)\n",
        "                rec1.append(recall)\n",
        "                F1.append(Fscore)\n",
        "            elif idx == 1:\n",
        "                prec2.append(precision)\n",
        "                rec2.append(recall)\n",
        "                F2.append(Fscore)\n",
        "            elif idx == 2:\n",
        "                prec3.append(precision)\n",
        "                rec3.append(recall)\n",
        "                F3.append(Fscore)\n",
        "            target_topi = prob_vectors[idx]\n",
        "            HT = get_HT(vectorized_target, target_topi, num_set_elements) # either a 1 or a 0\n",
        "            hit += HT\n",
        "            ndcg = get_NDCG(vectorized_target, target_topi, num_set_elements)\n",
        "            NDCG.append(ndcg)\n",
        "\n",
        "            # print(idx + 1, 'out of', len(output_vectors))\n",
        "            # print('ground truth', target_variable[1 + idx])\n",
        "            # print('prediction', prediction)\n",
        "            # print('recall: %s, precision: %s, f1: %s, correct: %s, ndcg: %s, HT: %s' % (recall, precision, Fscore, correct, ndcg, HT))\n",
        "        if hit == next_k_step:\n",
        "            n_hit += 1\n",
        "\n",
        "    if shouldPrint:\n",
        "        print('average precision of subsequent sets' + ': ' + str(np.mean(prec)) + ' with std: ' + str(np.std(prec)))\n",
        "        print('average recall' + ': ' + str(np.mean(rec)) + ' with std: ' + str(np.std(rec)))\n",
        "        print('average F score of subsequent sets' + ': ' + str(np.mean(F)) + ' with std: ' + str(np.std(F)))\n",
        "        print('average NDCG: ' + str(np.mean(NDCG)))\n",
        "        print('average hit rate: ' + str(n_hit / len(y_test)))\n",
        "        print('average inference time:' + str(np.mean(inference_time)))\n",
        "        print('max inference time:' + str(max(inference_time)))\n",
        "        print('min inference time:' + str(min(inference_time)))\n",
        "\n",
        "        # print('average precision of 1st' + ': ' + str(np.mean(prec1)) + ' with std: ' + str(np.std(prec1)))\n",
        "        # print('average recall of 1st' + ': ' + str(np.mean(rec1)) + ' with std: ' + str(np.std(rec1)))\n",
        "        # print('average F score of 1st' + ': ' + str(np.mean(F1)) + ' with std: ' + str(np.std(F1)))\n",
        "        # print('average precision of 2nd' + ': ' + str(np.mean(prec2)) + ' with std: ' + str(np.std(prec2)))\n",
        "        # print('average recall of 2nd' + ': ' + str(np.mean(rec2)) + ' with std: ' + str(np.std(rec2)))\n",
        "        # print('average F score of 2nd' + ': ' + str(np.mean(F2)) + ' with std: ' + str(np.std(F2)))\n",
        "        # print('average precision of 3rd' + ': ' + str(np.mean(prec3)) + ' with std: ' + str(np.std(prec3)))\n",
        "        # print('average recall of 3rd' + ': ' + str(np.mean(rec3)) + ' with std: ' + str(np.std(rec3)))\n",
        "        # print('average F score of 3rd' + ': ' + str(np.mean(F3)) + ' with std: ' + str(np.std(F3)))\n",
        "        print(\"rec, prec, ndcg, hr\", np.mean(rec), np.mean(prec), np.mean(NDCG), n_hit / len(y_test))\n",
        "    return np.mean(rec), np.mean(NDCG), n_hit / len(y_test), np.mean(prec), np.mean(F)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 2,
          "status": "ok",
          "timestamp": 1625141140510,
          "user": {
            "displayName": "Casper S",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgBWNg-LIJSRsGFg-KH-8zFdztG9qjoiJIbOsd4kA=s64",
            "userId": "09157385483834993232"
          },
          "user_tz": -120
        },
        "gather": {
          "logged": 1625155418098
        },
        "id": "zMejY92aZMdJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_random_prediction(y_test, output_size, next_k_step, num_set_elements):\n",
        "    prec = []\n",
        "    rec = []\n",
        "    F = []\n",
        "    prec1 = []\n",
        "    rec1 = []\n",
        "    F1 = []\n",
        "    prec2 = []\n",
        "    rec2 = []\n",
        "    F2 = []\n",
        "    prec3 = []\n",
        "    rec3 = []\n",
        "    F3 = []\n",
        "\n",
        "\n",
        "    NDCG = []\n",
        "    n_hit = 0\n",
        "    count = 0\n",
        "\n",
        "    for iter in range(len(y_test)):\n",
        "        target_variable = y_test[iter]\n",
        "\n",
        "        count += 1\n",
        "        hit = 0\n",
        "        # for each set\n",
        "        for idx in range(next_k_step):\n",
        "            # for idx in [2]:\n",
        "            vectorized_target = np.zeros(output_size)\n",
        "            for ii in target_variable[1 + idx]:\n",
        "                vectorized_target[ii] = 1\n",
        "\n",
        "            # random\n",
        "            random_output = np.zeros(output_size - 2)\n",
        "            random_output[:num_set_elements] = 1\n",
        "            np.random.shuffle(random_output)\n",
        "            vectorized_output = [0, *random_output, 0]\n",
        "\n",
        "            precision, recall, Fscore, correct = get_precision_recall_Fscore(vectorized_target, vectorized_output)\n",
        "            prec.append(precision)\n",
        "            rec.append(recall)\n",
        "            F.append(Fscore)\n",
        "            if idx == 0:\n",
        "                prec1.append(precision)\n",
        "                rec1.append(recall)\n",
        "                F1.append(Fscore)\n",
        "            elif idx == 1:\n",
        "                prec2.append(precision)\n",
        "                rec2.append(recall)\n",
        "                F2.append(Fscore)\n",
        "            elif idx == 2:\n",
        "                prec3.append(precision)\n",
        "                rec3.append(recall)\n",
        "                F3.append(Fscore)\n",
        "\n",
        "\n",
        "            prediction = []\n",
        "            for i, ele in enumerate(vectorized_output):\n",
        "                if ele == 1:\n",
        "                    prediction.append(i)\n",
        "\n",
        "            assert max(prediction) <= output_size - 2, \"random prediction too high!\" + str(prediction)\n",
        "            assert min(prediction) >= 1, \"random prediction too low!\" + str(prediction)\n",
        "\n",
        "            hit += get_HT(vectorized_target, prediction, num_set_elements) # either a 1 or a 0 if all elements in set are correct?\n",
        "            ndcg = get_NDCG(vectorized_target, prediction, num_set_elements)\n",
        "            NDCG.append(ndcg)\n",
        "        if hit == next_k_step:\n",
        "            n_hit += 1\n",
        "\n",
        "\n",
        "    print('average precision of subsequent sets' + ': ' + str(np.mean(prec)) + ' with std: ' + str(np.std(prec)))\n",
        "    print('average recall' + ': ' + str(np.mean(rec)) + ' with std: ' + str(np.std(rec)))\n",
        "    print('average F score of subsequent sets' + ': ' + str(np.mean(F)) + ' with std: ' + str(np.std(F)))\n",
        "    # print('average precision of 1st' + ': ' + str(np.mean(prec1)) + ' with std: ' + str(np.std(prec1)))\n",
        "    # print('average recall of 1st' + ': ' + str(np.mean(rec1)) + ' with std: ' + str(np.std(rec1)))\n",
        "    # print('average F score of 1st' + ': ' + str(np.mean(F1)) + ' with std: ' + str(np.std(F1)))\n",
        "    # print('average precision of 2nd' + ': ' + str(np.mean(prec2)) + ' with std: ' + str(np.std(prec2)))\n",
        "    # print('average recall of 2nd' + ': ' + str(np.mean(rec2)) + ' with std: ' + str(np.std(rec2)))\n",
        "    # print('average F score of 2nd' + ': ' + str(np.mean(F2)) + ' with std: ' + str(np.std(F2)))\n",
        "    # print('average precision of 3rd' + ': ' + str(np.mean(prec3)) + ' with std: ' + str(np.std(prec3)))\n",
        "    # print('average recall of 3rd' + ': ' + str(np.mean(rec3)) + ' with std: ' + str(np.std(rec3)))\n",
        "    # print('average F score of 3rd' + ': ' + str(np.mean(F3)) + ' with std: ' + str(np.std(F3)))\n",
        "    print('average NDCG: ' + str(np.mean(NDCG)))\n",
        "    print('average hit rate: ' + str(n_hit / len(y_test)))\n",
        "    print(\"rec, prec, ndcg, hr\",  \"&\", round(np.mean(rec), 4), \"&\", round(np.mean(prec), 4),  \"&\", round(np.mean(NDCG), 4),  \"&\", round((n_hit / len(y_test)), 4), \"\\\\\")\n",
        "    return np.mean(rec), np.mean(NDCG), n_hit / len(y_test), np.mean(prec), np.mean(F)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 3,
          "status": "ok",
          "timestamp": 1625141140511,
          "user": {
            "displayName": "Casper S",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgBWNg-LIJSRsGFg-KH-8zFdztG9qjoiJIbOsd4kA=s64",
            "userId": "09157385483834993232"
          },
          "user_tz": -120
        },
        "gather": {
          "logged": 1625155418297
        },
        "id": "O-k2-1WTx-8I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_top_frequent_prediction(y_test, output_size, next_k_step, num_set_elements, most_frequent_elements):\n",
        "    prec = []\n",
        "    rec = []\n",
        "    F = []\n",
        "    prec1 = []\n",
        "    rec1 = []\n",
        "    F1 = []\n",
        "    prec2 = []\n",
        "    rec2 = []\n",
        "    F2 = []\n",
        "    prec3 = []\n",
        "    rec3 = []\n",
        "    F3 = []\n",
        "\n",
        "    NDCG = []\n",
        "    n_hit = 0\n",
        "    count = 0\n",
        "\n",
        "    for iter in range(len(y_test)):\n",
        "        target_variable = y_test[iter]\n",
        "\n",
        "        count += 1\n",
        "        hit = 0\n",
        "        # for each set\n",
        "        for idx in range(next_k_step):\n",
        "            # for idx in [2]:\n",
        "            vectorized_target = np.zeros(output_size)\n",
        "            for ii in target_variable[1 + idx]:\n",
        "                vectorized_target[ii] = 1\n",
        "            \n",
        "            # top frequent prediction\n",
        "            vectorized_output = np.zeros(output_size)\n",
        "            for ii in most_frequent_elements:\n",
        "                vectorized_output[ii] = 1\n",
        "\n",
        "\n",
        "            precision, recall, Fscore, correct = get_precision_recall_Fscore(vectorized_target, vectorized_output)\n",
        "            prec.append(precision)\n",
        "            rec.append(recall)\n",
        "            F.append(Fscore)\n",
        "            if idx == 0:\n",
        "                prec1.append(precision)\n",
        "                rec1.append(recall)\n",
        "                F1.append(Fscore)\n",
        "            elif idx == 1:\n",
        "                prec2.append(precision)\n",
        "                rec2.append(recall)\n",
        "                F2.append(Fscore)\n",
        "            elif idx == 2:\n",
        "                prec3.append(precision)\n",
        "                rec3.append(recall)\n",
        "                F3.append(Fscore)\n",
        "\n",
        "            hit += get_HT(vectorized_target, most_frequent_elements, num_set_elements) # either a 1 or a 0 if all elements in set are correct?\n",
        "            ndcg = get_NDCG(vectorized_target, most_frequent_elements, num_set_elements)\n",
        "            NDCG.append(ndcg)\n",
        "        if hit == next_k_step:\n",
        "            n_hit += 1\n",
        "\n",
        "\n",
        "    print('average precision of subsequent sets' + ': ' + str(np.mean(prec)) + ' with std: ' + str(np.std(prec)))\n",
        "    print('average recall' + ': ' + str(np.mean(rec)) + ' with std: ' + str(np.std(rec)))\n",
        "    print('average F score of subsequent sets' + ': ' + str(np.mean(F)) + ' with std: ' + str(np.std(F)))\n",
        "    # print('average precision of 1st' + ': ' + str(np.mean(prec1)) + ' with std: ' + str(np.std(prec1)))\n",
        "    # print('average recall of 1st' + ': ' + str(np.mean(rec1)) + ' with std: ' + str(np.std(rec1)))\n",
        "    # print('average F score of 1st' + ': ' + str(np.mean(F1)) + ' with std: ' + str(np.std(F1)))\n",
        "    # print('average precision of 2nd' + ': ' + str(np.mean(prec2)) + ' with std: ' + str(np.std(prec2)))\n",
        "    # print('average recall of 2nd' + ': ' + str(np.mean(rec2)) + ' with std: ' + str(np.std(rec2)))\n",
        "    # print('average F score of 2nd' + ': ' + str(np.mean(F2)) + ' with std: ' + str(np.std(F2)))\n",
        "    # print('average precision of 3rd' + ': ' + str(np.mean(prec3)) + ' with std: ' + str(np.std(prec3)))\n",
        "    # print('average recall of 3rd' + ': ' + str(np.mean(rec3)) + ' with std: ' + str(np.std(rec3)))\n",
        "    # print('average F score of 3rd' + ': ' + str(np.mean(F3)) + ' with std: ' + str(np.std(F3)))\n",
        "    print('average NDCG: ' + str(np.mean(NDCG)))\n",
        "    print('average hit rate: ' + str(n_hit / len(y_test)))\n",
        "    print(\"rec, prec, ndcg, hr\",  \"&\", round(np.mean(rec), 4), \"&\", round(np.mean(prec), 4),  \"&\", round(np.mean(NDCG), 4),  \"&\", round((n_hit / len(y_test)), 4), \"\\\\\")\n",
        "    return np.mean(rec), np.mean(NDCG), n_hit / len(y_test), np.mean(prec), np.mean(F)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 3,
          "status": "ok",
          "timestamp": 1625141140839,
          "user": {
            "displayName": "Casper S",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgBWNg-LIJSRsGFg-KH-8zFdztG9qjoiJIbOsd4kA=s64",
            "userId": "09157385483834993232"
          },
          "user_tz": -120
        },
        "gather": {
          "logged": 1625155418479
        },
        "id": "-5rNWghj1ECs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_repeat_prediction(X_test, y_test, output_size, next_k_step, num_set_elements):\n",
        "    prec = []\n",
        "    rec = []\n",
        "    F = []\n",
        "    prec1 = []\n",
        "    rec1 = []\n",
        "    F1 = []\n",
        "    prec2 = []\n",
        "    rec2 = []\n",
        "    F2 = []\n",
        "    prec3 = []\n",
        "    rec3 = []\n",
        "    F3 = []\n",
        "\n",
        "    NDCG = []\n",
        "    n_hit = 0\n",
        "    count = 0\n",
        "\n",
        "    for iter in range(len(y_test)):\n",
        "        input_variable = X_test[iter]\n",
        "        target_variable = y_test[iter]\n",
        "\n",
        "        prediction = input_variable[-2]\n",
        "\n",
        "        count += 1\n",
        "        hit = 0\n",
        "        # for each set\n",
        "        for idx in range(next_k_step):\n",
        "            # for idx in [2]:\n",
        "            vectorized_target = np.zeros(output_size)\n",
        "            for ii in target_variable[1 + idx]:\n",
        "                vectorized_target[ii] = 1\n",
        "            \n",
        "            vectorized_output = np.zeros(output_size)\n",
        "            for ii in prediction:\n",
        "                vectorized_output[ii] = 1\n",
        "\n",
        "\n",
        "            precision, recall, Fscore, correct = get_precision_recall_Fscore(vectorized_target, vectorized_output)\n",
        "            prec.append(precision)\n",
        "            rec.append(recall)\n",
        "            F.append(Fscore)\n",
        "            if idx == 0:\n",
        "                prec1.append(precision)\n",
        "                rec1.append(recall)\n",
        "                F1.append(Fscore)\n",
        "            elif idx == 1:\n",
        "                prec2.append(precision)\n",
        "                rec2.append(recall)\n",
        "                F2.append(Fscore)\n",
        "            elif idx == 2:\n",
        "                prec3.append(precision)\n",
        "                rec3.append(recall)\n",
        "                F3.append(Fscore)\n",
        "            \n",
        "            hit += get_HT(vectorized_target, prediction, num_set_elements) # either a 1 or a 0 if all elements in set are correct?\n",
        "            ndcg = get_NDCG(vectorized_target, prediction, num_set_elements)\n",
        "            NDCG.append(ndcg)\n",
        "        if hit == next_k_step:\n",
        "            n_hit += 1\n",
        "\n",
        "\n",
        "    print('average precision of subsequent sets' + ': ' + str(np.mean(prec)) + ' with std: ' + str(np.std(prec)))\n",
        "    print('average recall' + ': ' + str(np.mean(rec)) + ' with std: ' + str(np.std(rec)))\n",
        "    print('average F score of subsequent sets' + ': ' + str(np.mean(F)) + ' with std: ' + str(np.std(F)))\n",
        "    print('average NDCG: ' + str(np.mean(NDCG)))\n",
        "    print('average hit rate: ' + str(n_hit / len(y_test)))\n",
        "    print(\"rec, prec, ndcg, hr\",  \"&\", round(np.mean(rec), 4), \"&\", round(np.mean(prec), 4),  \"&\", round(np.mean(NDCG), 4),  \"&\", round((n_hit / len(y_test)), 4), \"\\\\\")\n",
        "    return np.mean(rec), np.mean(NDCG), n_hit / len(y_test), np.mean(prec), np.mean(F)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 3,
          "status": "ok",
          "timestamp": 1625141140840,
          "user": {
            "displayName": "Casper S",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgBWNg-LIJSRsGFg-KH-8zFdztG9qjoiJIbOsd4kA=s64",
            "userId": "09157385483834993232"
          },
          "user_tz": -120
        },
        "gather": {
          "logged": 1625155418565
        },
        "id": "j2iszEAaklYO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(path, history_file_name, future_file_name, input_size):\n",
        "    print(\"Loading data from disk...\")\n",
        "    freq_max = 200\n",
        "    X = []\n",
        "    X_test = []\n",
        "    X_frequency_init = np.zeros(input_size + 2)\n",
        "\n",
        "    set_sizes = []\n",
        "    \n",
        "    with open(path + \"data/\" + history_file_name, 'r') as csvfile:\n",
        "        reader = csv.DictReader(csvfile)\n",
        "        prevQueryId = currentQueryId = None\n",
        "        tmp = []\n",
        "        for i, row in enumerate(reader):\n",
        "            if i == 0:\n",
        "                prevQueryId = currentQueryId = row['queryId']\n",
        "            currentQueryId = row['queryId']\n",
        "            if currentQueryId != prevQueryId:\n",
        "                X.append(np.sort(tmp))\n",
        "                tmp = []\n",
        "            tmp.append(int(row['partialQEPId']))\n",
        "            \n",
        "            set_sizes.append(len(tmp))\n",
        "\n",
        "            prevQueryId = currentQueryId\n",
        "    \n",
        "#     print(\"X first 100:\", X[:100])\n",
        "#     print(\"X last 100:\", X[-100:])\n",
        "\n",
        "    with open(path + \"data/\" + future_file_name, 'r') as csvfile:\n",
        "        reader = csv.DictReader(csvfile)\n",
        "        prevQueryId = currentQueryId = None\n",
        "        tmp = []\n",
        "        for i, row in enumerate(reader):\n",
        "            if i == 0:\n",
        "                prevQueryId = currentQueryId = row['queryId']\n",
        "            currentQueryId = row['queryId']\n",
        "            if currentQueryId != prevQueryId:\n",
        "                X_test.append(np.sort(tmp))\n",
        "                tmp = []\n",
        "            tmp.append(int(row['partialQEPId']))\n",
        "            set_sizes.append(len(tmp))\n",
        "            prevQueryId = currentQueryId\n",
        "    print(\"X_test first 100:\", X_test[:100])\n",
        "    print(\"X_test last 100:\", X_test[-100:])\n",
        "\n",
        "\n",
        "    print(\"Maximum set size:\", max(set_sizes))\n",
        "    print(\"Average set size:\", np.mean(set_sizes))\n",
        "\n",
        "    return X, X_test, input_size + 2, X_frequency_init\n",
        "\n",
        "\n",
        "def load_dependency_graph(path, filename):\n",
        "    graph = {}\n",
        "    with open(path + \"data/\" + filename, mode='r') as csvfile:\n",
        "        reader = csv.DictReader(csvfile, quotechar='\"', delimiter=';')\n",
        "        for row in reader:\n",
        "            if len(row['impliedPartialQEPs']) > 0:\n",
        "                graph[row['partialQEP']] = [int(s) for s in row['impliedPartialQEPs'].split(',')]\n",
        "            else:\n",
        "                graph[row['partialQEP']] = []\n",
        "    return graph\n",
        "\n",
        "def get_frequency_vector(X, X_dim):\n",
        "    result_vector = np.zeros(X_dim, dtype=int)\n",
        "    for i in X:\n",
        "        result_vector[i] += 1\n",
        "    return result_vector\n",
        "\n",
        "\n",
        "def sliding_windows(X, seq_length, next_k_step):\n",
        "    X_train = []\n",
        "    y_train = []\n",
        "\n",
        "    for i in range(len(X)-(seq_length+next_k_step)):\n",
        "        _X_train = [[-1], *X[i:(i+seq_length)]]\n",
        "        _y_train = [[-1], *X[(i+seq_length):(i+seq_length+next_k_step)]]\n",
        "        _X_train.append([-1])\n",
        "        _y_train.append([-1])\n",
        "        X_train.append(_X_train)\n",
        "        y_train.append(_y_train)\n",
        "\n",
        "    return X_train, y_train\n",
        "\n",
        "def cv_split(X, n_folds, fold_size, seq_length, next_k_step):\n",
        "    K = fold_size\n",
        "    res = set()\n",
        "    for _ in range(n_folds):\n",
        "        temp = random.randint(0, len(X) - K) \n",
        "        while any(temp >= idx and temp <= idx + K for idx in res):\n",
        "            temp = random.randint(0, len(X) - K) \n",
        "        res.add(temp)\n",
        "    res = [(idx, idx + K) for idx in res]\n",
        "\n",
        "    print(\"The n_folds of non-overlapping random ranges are : \" + str(res))\n",
        "\n",
        "    # folds = [X[interval[0]:interval[1]] for interval in res]\n",
        "\n",
        "    # [(32611, 34611), (111787, 113787), (72365, 74365), (98960, 100960), (6677, 8677)]\n",
        "\n",
        "    folds = [X[13853:16853]] # for hyperparameter tuning, 1 fold, length 3000\n",
        "    folds = [[fold[:int(len(fold)*0.8)], fold[int(len(fold)*0.8):]] for fold in folds]\n",
        "\n",
        "    result = []\n",
        "    for fold in folds:\n",
        "        fold_X_train, fold_y_train = sliding_windows(fold[0], seq_length, next_k_step)\n",
        "        fold_X_val, fold_y_val = sliding_windows(fold[1], seq_length, next_k_step)\n",
        "        result.append([[fold_X_train, fold_y_train], [fold_X_val, fold_y_val]])\n",
        "    print(\"Number of training/validation folds:\", len(result))\n",
        "    print(\"Number of training instances per fold:\", len(result[0][0][0])) # first fold, training, X\n",
        "    print(\"Number of validation instances per fold:\", len(result[0][1][0])) # first fold, validation, X\n",
        "    print(\"CV_split shape\", np.array(result).shape)\n",
        "    return result\n",
        "\n",
        "\n",
        "def sample_pairs(input_sequence, seq_length, next_k_step, n_pairs):\n",
        "    X = []\n",
        "    y = []\n",
        "    for _ in range(n_pairs):\n",
        "        idx = random.randint(0, len(input_sequence) - (seq_length + next_k_step))\n",
        "        X.append([[-1], *input_sequence[idx:idx + seq_length], [-1]])\n",
        "        y.append([[-1], *input_sequence[idx + seq_length:idx + seq_length + next_k_step], [-1]])\n",
        "    return X, y"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 4,
          "status": "ok",
          "timestamp": 1625141140841,
          "user": {
            "displayName": "Casper S",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgBWNg-LIJSRsGFg-KH-8zFdztG9qjoiJIbOsd4kA=s64",
            "userId": "09157385483834993232"
          },
          "user_tz": -120
        },
        "gather": {
          "logged": 1625155418741
        },
        "id": "kMqVAq07ZPA7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "\n",
        "    dataset = \"phonelabs\" # \"iot\" | \"phonelabs\"\n",
        "\n",
        "    if dataset == \"iot\":\n",
        "        history_file_name, future_file_name = ['iot_output_first80percent_6000.csv', 'iot_output_last20percent_6000.csv']\n",
        "        graph_file_name = \"iot_output_dependency_graph_6000.csv\"\n",
        "        if use_dependency_graph:\n",
        "            model_version = '_iot_0.001'\n",
        "        else:\n",
        "            model_version = '_iot_no_graph_0.001'\n",
        "        dict_size = 5801\n",
        "\n",
        "    elif dataset == \"phonelabs\":\n",
        "        history_file_name, future_file_name = ['phonelabs_output_first80percent_5000.csv', 'phonelabs_output_last20percent_5000.csv']\n",
        "        graph_file_name = \"phonelabs_output_dependency_graph_5000.csv\"\n",
        "        if use_dependency_graph:\n",
        "            model_version = '_phonelabs_0.001'\n",
        "        else:\n",
        "            model_version = '_phonelabs_no_graph_0.001'\n",
        "        dict_size = 4729 # dictionary size, number of possible partial QEPs\n",
        "    \n",
        "    training = False # True for training mode, False for test mode\n",
        "    path = \"./\"\n",
        "    directory = './models/'\n",
        "    \n",
        "    seq_length = 20 # length of the input sequences we train on\n",
        "    next_k_step = 2 # next k steps to predict\n",
        "    n_training_pairs = 20000 # number of training pairs\n",
        "    n_testing_pairs = 5000 # number of testing pairs\n",
        "    num_set_elements_validation = 5 # number of set elements to predict for validaiton set during training\n",
        "\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "    print(torch.cuda.get_device_name(0))\n",
        "    print(\"Using dependency graph:\", use_dependency_graph)\n",
        "    print(\"Files:\", history_file_name, future_file_name)\n",
        "    print(\"Seq_length: \" + str(seq_length) +\", Next_k_step: \" + str(next_k_step))\n",
        "    print(\"Training pairs: \" + str(n_training_pairs) + \", Testing pairs: \" + str(n_testing_pairs))\n",
        "    \n",
        "    X, X_future, X_dim, X_frequency_init = load_data(path, history_file_name, future_file_name, dict_size)\n",
        "    \n",
        "    implied_elements = load_dependency_graph(path, graph_file_name)\n",
        "    print(\"Dependencies:\", implied_elements)\n",
        "\n",
        "    X_freq = get_frequency_vector(X, X_dim)\n",
        "    print(\"X_freq:\", X_freq)\n",
        "    print(\"X_dim:\", X_dim)\n",
        "\n",
        "    weights = np.zeros(X_dim)\n",
        "    max_freq = max(X_freq)\n",
        "    \n",
        "    print('Maximum feature frequency:', max_freq)\n",
        "    for i in range(len(X_freq)):\n",
        "        if X_freq[i] > 0:\n",
        "            weights[i] = max_freq / X_freq[i]\n",
        "        else:\n",
        "            weights[i] = 0\n",
        "\n",
        "    print(\"Weights:\", weights)\n",
        "\n",
        "\n",
        "    X_train, y_train = sample_pairs(X, seq_length, next_k_step, n_training_pairs)\n",
        "    X_val, y_val = sample_pairs(X_future, seq_length, next_k_step, n_testing_pairs)\n",
        "    X_test, y_test = sample_pairs(X_future, seq_length, next_k_step, n_testing_pairs)\n",
        "\n",
        "    print(\"train X  has:\", len(X_train) , \"pairs\")\n",
        "    print(\"train y  has:\", len(y_train) , \"pairs\")\n",
        "    print(\"val X  has:\", len(X_val) , \"pairs\")\n",
        "    print(\"val y  has:\", len(y_val) , \"pairs\")\n",
        "    print(\"test X  has:\", len(X_test) , \"pairs\")\n",
        "    print(\"test y  has:\", len(y_test) , \"pairs\")\n",
        "\n",
        "    print(X_train[0])\n",
        "    print(y_train[0])\n",
        "    print(X_train[1])\n",
        "    print(y_train[1])\n",
        "\n",
        "\n",
        "    print(\"train X max length\", len(max(X_train, key=len)))\n",
        "    print(\"train y max length\", len(max(y_train, key=len)))\n",
        "    print(\"train X min length\", len(min(X_train, key=len)))\n",
        "    print(\"train y min length\", len(min(y_train, key=len)))\n",
        "    \n",
        "    print(\"test X max length\", len(max(X_test, key=len)))\n",
        "    print(\"test y max length\", len(max(y_test, key=len)))\n",
        "    print(\"test X min length\", len(min(X_test, key=len)))\n",
        "    print(\"test y min length\", len(min(y_test, key=len)))\n",
        "\n",
        "    encoder1 = EncoderRNN(X_dim, hidden_size, num_layers)\n",
        "    attn_decoder1 = AttnDecoderRNN(hidden_size, X_dim, num_layers, dropout_p=0.1)\n",
        "\n",
        "    if use_cuda:\n",
        "        encoder1 = encoder1.cuda()\n",
        "        attn_decoder1 = attn_decoder1.cuda()\n",
        "\n",
        "    if training:\n",
        "        if atten_decoder:\n",
        "            trainIters(X_train, y_train, X_val, y_val, X_dim, encoder1, attn_decoder1, model_version, weights,\n",
        "                       next_k_step, num_iter, num_set_elements_validation, implied_elements, print_every=print_val)\n",
        "\n",
        "    else:\n",
        "        for i in [5, 10, 15, 20]: # number of elements predict for each set\n",
        "            valid_recall = []\n",
        "            valid_ndcg = []\n",
        "            valid_hr = []\n",
        "            valid_prec = []\n",
        "            valid_f1 = []\n",
        "            recall_list = []\n",
        "            ndcg_list = []\n",
        "            hr_list = []\n",
        "            prec_list = []\n",
        "            f1_list = []\n",
        "            print('k = ' + str(i))\n",
        "\n",
        "            print(\"Random prediction\")\n",
        "            evaluate_random_prediction(y_test, X_dim, next_k_step, i)\n",
        "\n",
        "            print(\"Top k frequent prediction\")\n",
        "            most_frequent_elements = X_freq.argsort()[::-1][:i]\n",
        "            evaluate_top_frequent_prediction(y_test, X_dim, next_k_step, i, most_frequent_elements)\n",
        "\n",
        "            print(\"Repeat prediction\")\n",
        "            evaluate_repeat_prediction(X_test, y_test, X_dim, next_k_step, i)\n",
        "\n",
        "            for model_epoch in range(5):\n",
        "                print('Epoch: ', model_epoch)\n",
        "                encoder_pathes = './models/encoder' + str(model_version) + '_model_epoch' + str(model_epoch)\n",
        "                decoder_pathes = './models/decoder' + str(model_version) + '_model_epoch' + str(model_epoch)\n",
        "\n",
        "                encoder_instance = torch.load(encoder_pathes)\n",
        "                decoder_instance = torch.load(decoder_pathes)\n",
        "\n",
        "                recall, ndcg, hr, prec, f1  = evaluate(X_test, y_test, encoder_instance, decoder_instance, X_dim, next_k_step, i, implied_elements, shouldPrint=True)\n",
        "                \n",
        "                recall_list.append(round(recall, 4))\n",
        "                ndcg_list.append(round(ndcg, 4))\n",
        "                hr_list.append(round(hr, 4))\n",
        "                prec_list.append(round(prec, 4))\n",
        "                f1_list.append(round(f1, 4))\n",
        "                \n",
        "                \n",
        "            # valid_recall = np.asarray(valid_recall)\n",
        "            # valid_ndcg = np.asarray(valid_ndcg)\n",
        "            # valid_hr = np.asarray(valid_hr)\n",
        "            # idx1 = valid_recall.argsort()[::-1][0]\n",
        "            # idx2 = valid_ndcg.argsort()[::-1][0]\n",
        "            # idx3 = valid_hr.argsort()[::-1][0]\n",
        "            # print('max valid recall results:')\n",
        "            # print('Epoch: ', idx1)\n",
        "            # print('recall: ', recall_list[idx1])\n",
        "            # print('ndcg: ', ndcg_list[idx1])\n",
        "            # print('phr: ', hr_list[idx1])\n",
        "\n",
        "            # print('max valid ndcg results:')\n",
        "            # print('Epoch: ', idx2)\n",
        "            # print('recall: ', recall_list[idx2])\n",
        "            # print('ndcg: ', ndcg_list[idx2])\n",
        "            # print('phr: ', hr_list[idx2])\n",
        "\n",
        "            # print('max valid phr results:')\n",
        "            # print('Epoch: ', idx3)\n",
        "            # print('recall: ', recall_list[idx3])\n",
        "            # print('ndcg: ', ndcg_list[idx3])\n",
        "            # print('phr: ', hr_list[idx3])\n",
        "\n",
        "\n",
        "            # print('Validation recall:', valid_recall)\n",
        "            # print('Validation ndcg:', valid_ndcg)\n",
        "            # print('Validation hit:', valid_hr)\n",
        "            # print('Validation precision:', valid_prec)\n",
        "            # print('Validation f1:', valid_f1)\n",
        "            print('Test recall:', recall_list)\n",
        "            print('Test ndcg:', ndcg_list)\n",
        "            print('Test hit:', hr_list)\n",
        "            print('Test precision:', prec_list)\n",
        "            print('Test f1:', f1_list)\n",
        "\n",
        "\n",
        "            lc = pd.DataFrame({\"Test recall\": recall_list, \"Test precision\": prec_list, \"Test ndcg\": ndcg_list, \"Test hit\": hr_list, \"Test f1\": f1_list})\n",
        "            title = \"Test, k = \" + str(i)\n",
        "            lc.plot(lw=2, title=title);\n",
        "            plt.xlabel('epochs');\n",
        "            print(title)\n",
        "            print(lc.apply(minMax))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 421,
          "status": "ok",
          "timestamp": 1625141234721,
          "user": {
            "displayName": "Casper S",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgBWNg-LIJSRsGFg-KH-8zFdztG9qjoiJIbOsd4kA=s64",
            "userId": "09157385483834993232"
          },
          "user_tz": -120
        },
        "gather": {
          "logged": 1625155418930
        },
        "id": "MiDyn9bpZgMq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXvqKVr8ZhYN",
        "outputId": "3c146d1c-b6b7-465f-e6ce-87827f81d7fd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyN2+bRyzXbX6hr8iiMtDxcP",
      "collapsed_sections": [],
      "name": "DeepLearningForSQLOperators.ipynb",
      "provenance": []
    },
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}